---
layout: post
title:  "Real-World Hurdles with Millions of Players"
date:   2014-11-13
author: "Chris Chalfant"
---

*No slides for this one yet.*

Chris Gallinaro and Colin Shirley, Electronic Arts
The Simpsons: Tapped Out

2.5 years. 3 developers/operations guys at the beginning.

ios,android,kindle town building game.
new client rls/content every 2 weeks.

Stats
-----
* peak user count per day 7.2M
* request rates exceeding 20k/sec
* 5k+ reads dynamodb
* 2.5k writes
* 80k ec2 hours
* 150 instances at peak
* 4M new s3 objects/day
* 30M reads/day, 30M writes/day

Launch History
--------------
* game server outsourced
* predicted 10M daily users

Issues
------
* poor perf
* 40 db shards - would query each shard looking for a record
* bad data integrity 2% userbase / day corruption
* no visibility
* complicated deployments

As a Result...
--------------
* game delisted from app store (3/2012)
* EA took over development
* relaunched in (8/2012)

Starting Arch
-------------
* elb
* haproxy
* game servers
* memcached
* s3/mysql (120 m3.xlarge dbs: 40 master, 40 readslave, 40 backup)

ec2 instances were manually managed and configured (no autoscale).
db cluster was **~1M USD/year**

Goal: Zero Downtime
-------------------
All game state was cached locally on a game server so players
had to be sticky. This was managed by HAProxy hashing. So hard
to add/remove instances.
Their error screen actually got parodied on the tv show.
Deployments bounced all active players and forced re-login.
Went stateless with ElastiCache by moving local data off.
Allowed to remove HAProxy.

Auth stability was bad (external ea-wide auth service).
Added mechanism to detect failures and bypass to backup auth.

Goal: mysql -> dynamodb
-----------------------

DB was not operated by game team
recurring perf issues
game team had little RDBMS experience
lots of org overhead to fix/improve
hard part was moving data w/ no downtime
step one: replicate all reads & writes to new datastore
step two: manually copy remaining data
step three: verify data and remove old datastore
took three months to do migration.
now data layer owned by game team.
now use redshift and data pipeline to do analytics.
90% cost reduction!

Goal: Autoscale
-----------------
on shutdown must persist telemetry and game state
did load tests to figure auto-scaling criteria
but many tweaks after launch (based on cpu now)
auto-scaling saved bigtime on instance hours
110k hours to 70k hours
They use blue-green dep with elastic beanstalk

D'oh: Whacking Day 2013
-----------------------
* many more friend events triggered by 100s of snake eggs
* dynamodb throughput increased 10x due to more friend events
* added dynamo autoscaling

D'oh: Push Notifications
------------------------
* usually: shard userbase into 24 chunks and send one chunk every hour over a day.
* marketing didn't use the usualy system, notifications went to every player, all over the world, at once.
* players joined all at once and autoscaling could not handle it.

Now
---
* one-man deployments
* one-man incident investigations
* game team has full control
* they benefit from ever-improving aws featureset
